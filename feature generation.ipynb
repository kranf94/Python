{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Things to try: (for tree-based model)\n",
    "- item_price: decimal part, item_price_bin\n",
    "- categorical variable interation: city&item_category, city&item_price_bin, item_category&item_price_bin\n",
    "- lag features\n",
    "- mean encoding\n",
    "- knn features\n",
    "- prophet forecasting\n",
    "- word embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References:\n",
    "- BOW: http://scikit-learn.org/stable/modules/feature_extraction.html#vectorizing-a-large-text-corpus-with-the-hashing-trick\n",
    "- Word2vec: http://mccormickml.com/2016/04/12/googles-pretrained-word2vec-model-in-python/\n",
    "- List of Pretrained Word Embeddings: http://ahogrammer.com/2017/01/20/the-list-of-pretrained-word-embeddings/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature extration on text: item and item_cats (using word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostRegressor\n",
    "from tqdm import tqdm_notebook\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import feather\n",
    "import pyarrow\n",
    "hol = pyarrow.feather.read_feather('hol.feather')\n",
    "it_en = pyarrow.feather.read_feather('it_en.feather')\n",
    "itc_en = pyarrow.feather.read_feather('itc_en.feather')\n",
    "sh_en = pyarrow.feather.read_feather('sh_en.feather')\n",
    "coordinates = pd.read_csv('coordinates.csv')\n",
    "city_name_eng = pd.read_csv('city_name_eng.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import nltk.data\n",
    "from nltk.corpus import stopwords  # if not installed, do nltk.download('stopwords')\n",
    "def sentence_to_wordlist (sentence, remove_stopwords=False ):\n",
    "\n",
    "    review_text = re.sub(\"[^a-zA-Z]\",\" \", sentence)\n",
    "\n",
    "        # 3. Convert words to lower case and split them\n",
    "    words = review_text.lower().split()\n",
    "\n",
    "        # 4. Optionally remove stop words (false by default)\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "\n",
    "        # 5. Return a list of words\n",
    "    return(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import word2vec\n",
    "from gensim.models import Phrases\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "it_en_word_embed_mean = []\n",
    "it_en_word_embed_sum = []\n",
    "for en_it_name in it_en.iloc[:,3]:\n",
    "    words = sentence_to_wordlist(en_it_name,True)\n",
    "    if any(word in model.vocab for word in words):\n",
    "        words_vec_mean = np.mean(np.vstack([model[word] for word in words if word in model.vocab]),axis=0)\n",
    "        words_vec_sum = np.sum(np.vstack([model[word] for word in words if word in model.vocab]),axis=0)\n",
    "    else: \n",
    "        words_vec_mean = np.zeros((300,))\n",
    "        words_vec_sum = np.zeros((300,))\n",
    "    it_en_word_embed_mean.append(words_vec_mean)\n",
    "    it_en_word_embed_sum.append(words_vec_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "index_cols1 = ['it_en_word_embed_mean' + str(a) for a in range(1,301)]\n",
    "index_cols2 = ['it_en_word_embed_sum' + str(a) for a in range(1,301)]\n",
    "it_en_word_embed_mean = pd.DataFrame(np.vstack(it_en_word_embed_mean), columns = index_cols1,dtype=np.float32)\n",
    "it_en_word_embed_sum = pd.DataFrame(np.vstack(it_en_word_embed_sum), columns = index_cols2,dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "itc_en_word_embed_mean = []\n",
    "itc_en_word_embed_sum = []\n",
    "for en_itc_name in itc_en.iloc[:,2]:\n",
    "    words = sentence_to_wordlist(en_itc_name,True)\n",
    "    if any(word in model.vocab for word in words):\n",
    "        words_vec_mean = np.mean(np.vstack([model[word] for word in words if word in model.vocab]),axis=0)\n",
    "        words_vec_sum = np.sum(np.vstack([model[word] for word in words if word in model.vocab]),axis=0)\n",
    "    else: \n",
    "        words_vec_mean = np.zeros((300,))\n",
    "        words_vec_sum = np.zeros((300,))\n",
    "    itc_en_word_embed_mean.append(words_vec_mean)\n",
    "    itc_en_word_embed_sum.append(words_vec_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "index_cols3 = ['itc_en_word_embed_mean' + str(a) for a in range(1,301)]\n",
    "index_cols4 = ['itc_en_word_embed_sum' + str(a) for a in range(1,301)]\n",
    "itc_en_word_embed_mean = pd.DataFrame(np.vstack(itc_en_word_embed_mean), columns = index_cols3,dtype=np.float32)\n",
    "itc_en_word_embed_sum = pd.DataFrame(np.vstack(itc_en_word_embed_sum), columns = index_cols4,dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add word embedding features to raw tables\n",
    "it_en_with_word_embed = pd.concat([it_en.reset_index(drop=True),it_en_word_embed_mean.reset_index(drop=True),it_en_word_embed_sum.reset_index(drop=True)],axis=1)\n",
    "itc_en_with_word_embed = pd.concat([itc_en.reset_index(drop=True),itc_en_word_embed_mean.reset_index(drop=True),itc_en_word_embed_sum.reset_index(drop=True)],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# it_en_with_word_embed.to_csv('it_en_with_word_embed.csv',index=False) may cause problem in first column (Russian)\n",
    "# itc_en_with_word_embed.to_csv('itc_en_with_word_embed.csv',index=False) may cause problem in first column (Russian)\n",
    "pyarrow.feather.write_feather(it_en_with_word_embed,'it_en_with_word_embed.feather')\n",
    "pyarrow.feather.write_feather(itc_en_with_word_embed,'itc_en_with_word_embed.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
